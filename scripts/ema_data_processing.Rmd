---
title: "ema_brain"
author: "eugenia giampetruzzi"
date: "2025-12-06"
output: html_document
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# raw data review

*emo = daily emotion data (positive & negative affect)*

*se = daily significant event data*

*sleep = daily sleep data (surveyed only in morning prompts)*

## dates, times, ids

```{r}
library(tidyverse)
library(lubridate)

# load data
emo_path   <- "/Users/eu/Library/CloudStorage/OneDrive-Stanford/Research Projects/9 - EMA-Brain/EMA-GitHub/Data/Raw/2_daily_emo_total_data.csv"
se_path    <- "/Users/eu/Library/CloudStorage/OneDrive-Stanford/Research Projects/9 - EMA-Brain/EMA-GitHub/Data/Raw/2_daily_se_total_data.csv"
sleep_path <- "/Users/eu/Library/CloudStorage/OneDrive-Stanford/Research Projects/9 - EMA-Brain/EMA-GitHub/Data/Raw/2_daily_sleep_total_data.csv"

df_emo   <- read.csv(emo_path, stringsAsFactors = FALSE)
df_se    <- read.csv(se_path, stringsAsFactors = FALSE)
df_sleep <- read.csv(sleep_path, stringsAsFactors = FALSE)

print(paste("loaded emo rows:", nrow(df_emo)))
print(paste("loaded se rows:", nrow(df_se)))
print(paste("loaded sleep rows:", nrow(df_sleep)))

# date checks
print("---------------------------------------------------------")
print("date format inconsistencies")
print("---------------------------------------------------------")

print("sample emo dates:")
print(head(unique(df_emo$emo_trigger_date), 10))

has_slashes <- any(grepl("/", df_emo$emo_trigger_date))
has_dashes  <- any(grepl("-", df_emo$emo_trigger_date))

if(has_slashes & has_dashes) {
  print("contains both 'y-m-d' and 'm/d/y'")
} else {
  print(paste("emo file format consistent? slashes:", has_slashes, "| dashes:", has_dashes))
}

# time checks
print("---------------------------------------------------------")
print("time format inconsistencies")
print("---------------------------------------------------------")

raw_times <- df_emo$emo_trigger_time
numeric_times <- raw_times[!grepl(":", raw_times) & raw_times != ""]

if(length(numeric_times) > 0) {
  print(paste("found", length(numeric_times), "timestamps in raw seconds"))
  print("examples:")
  print(head(numeric_times, 3))
} else {
  print("all times in hh:mm:ss format")
}

# trigger time match
print("---------------------------------------------------------")
print("trigger time drift (emo vs se)")
print("---------------------------------------------------------")

times_emo <- unique(df_emo$emo_trigger_time)
times_se  <- unique(df_se$se_trigger_time)

matches    <- intersect(times_emo, times_se)
mismatches <- setdiff(times_emo, times_se)

print(paste("unique emo times:", length(times_emo)))
print(paste("unique se times:", length(times_se)))
print(paste("exact matches:", length(matches)))

if(length(matches) < length(times_emo)) {
  print("trigger times do not match exactly")
  print("example mismatches:")
  print(head(mismatches, 5))
}

# id irregularities
print("---------------------------------------------------------")
print("id irregularities")
print("---------------------------------------------------------")

weird_ids <- unique(df_emo$ELS_ID[grepl("\\.", df_emo$ELS_ID)])

if(length(weird_ids) > 0) {
  print("ids with decimals:")
  print(weird_ids)
} else {
  print("no decimal ids found")
}

# duplicate prompts
print("---------------------------------------------------------")
print("duplicate prompts")
print("---------------------------------------------------------")

duplicates <- df_emo %>%
  group_by(ELS_ID, emo_trigger_date, emo_trigger_time) %>%
  summarize(count = n(), .groups = "drop") %>%
  filter(count > 1)

if(nrow(duplicates) > 0) {
  print(paste("found", nrow(duplicates), "duplicate prompts"))
  print("examples:")
  print(head(duplicates))
} else {
  print("no exact duplicates found")
}

print("---------------------------------------------------------")
print("diagnosis complete")
print("---------------------------------------------------------")
```

### summary

review of the raw ema files (emo, se, sleep) identified five issues:

**1. time format inconsistency**

timestamps appeared in two formats within the same column: standard
hh:mm:ss strings (e.g., 20:00:00) and raw seconds since midnight (e.g.,
24301)

**2. date format inconsistency**

date strings varied across files, mixing short-year formats (6/7/18),
full-year formats (13/07/2018), and different delimiters

**3. trigger time mismatch across files**

the same prompt often appeared with different trigger times across the
emo and se files (for example, 57600 vs 16:00:00)

**4. subject id irregularities**

some ids included decimal suffixes (e.g., 92.2, 174.2)

**5. duplicate prompt entries**

certain participants had multiple records for the same prompt timestamp

## prompt drift check

```{r}
# helper functions
clean_time <- function(x) {
  sapply(x, function(t) {
    if (is.na(t) || t == "") return(NA)
    if (grepl("^[0-9]+$", t)) {
      sec <- as.numeric(t)
      sprintf("%02d:%02d:%02d", sec %/% 3600, (sec %% 3600) %/% 60, sec %% 60)
    } else {
      tryCatch(
        format(parse_date_time(t, c("HMS", "HM")), "%H:%M:%S"),
        error = function(e) t
      )
    }
  })
}

clean_date <- function(x) {
  format(
    parse_date_time(x, orders = c("dmy", "mdy", "ymd", "Ymd", "dmY")),
    "%Y-%m-%d"
  )
}

get_session <- function(time_str) {
  h <- hour(hms(time_str))
  case_when(
    h >= 4  & h < 14 ~ "morning",
    h >= 14 & h < 18 ~ "afternoon",
    h >= 18         ~ "evening",
    TRUE            ~ NA_character_
  )
}

# prep emo for merge
emo_prep <- df_emo %>%
  mutate(
    clean_id     = sub("\\.2$", "", as.character(ELS_ID)),
    date         = clean_date(emo_trigger_date),
    time         = clean_time(emo_trigger_time),
    session      = get_session(time),
    datetime_emo = ymd_hms(paste(date, time))
  ) %>%
  select(clean_id, original_id = ELS_ID, date, session, datetime_emo)

# prep se for merge
se_prep <- df_se %>%
  mutate(
    clean_id    = sub("\\.2$", "", as.character(ELS_ID)),
    date        = clean_date(se_trigger_date),
    time        = clean_time(se_trigger_time),
    session     = get_session(time),
    datetime_se = ymd_hms(paste(date, time))
  ) %>%
  select(clean_id, date, session, datetime_se)

# id collision check (e.g., 92 vs 92.2 on same day/session)
message("--- checking id collisions across id versions ---")

collisions <- emo_prep %>%
  group_by(clean_id, date, session) %>%
  summarize(
    unique_original_ids = n_distinct(original_id),
    ids_present         = paste(unique(original_id), collapse = ", "),
    .groups             = "drop"
  ) %>%
  filter(unique_original_ids > 1)

if (nrow(collisions) > 0) {
  message("warning: overlapping data from multiple id versions")
  print(head(collisions))
} else {
  message("no date/session overlaps across id versions")
}

# merge and calculate drift
merged_check <- full_join(
  emo_prep,
  se_prep,
  by = c("clean_id", "date", "session")
) %>%
  mutate(
    drift_seconds = abs(as.numeric(difftime(datetime_emo, datetime_se, units = "secs"))),
    drift_minutes = drift_seconds / 60
  )

# drift summary
message("--- drift statistics (seconds) ---")
print(summary(merged_check$drift_seconds))

perfect_matches <- sum(merged_check$drift_seconds == 0, na.rm = TRUE)
message(paste("perfect matches (0s drift):", perfect_matches))

close_enough <- sum(merged_check$drift_seconds < 300, na.rm = TRUE)
message(paste("matches within 5 minutes:", close_enough))

major_fails <- merged_check %>%
  filter(drift_seconds > 3600) %>%
  select(clean_id, date, session, datetime_emo, datetime_se, drift_minutes)

message(paste("major mismatches (> 1 hour drift):", nrow(major_fails)))

if (nrow(major_fails) > 0) {
  message("example major mismatches:")
  print(head(major_fails))
}

# histogram of drift (log scale; excludes nas)
hist(
  log10(merged_check$drift_seconds + 1),
  main = "log-drift between emo and se trigger times",
  xlab = "log10(seconds): 0 = perfect match"
)
```

### summary

after standardizing timestamps, we examined the difference between the
emo and se trigger times for each matching id × date × session entry.
overall drift was very small for most prompts. the median and 75th
percentile were both 0 seconds, and the mean drift was 17.23 seconds,
indicating near-perfect alignment across files. however, a small subset
of entries showed large discrepancies.

**drift distribution (seconds):**

\- min = 0\
- median = 0\
- mean = 17.23\
- 3rd quartile = 1\
- max = 10800 (180 minutes)\
- missing drift values (na) = 129 (missing data explored at next step)

**sessions with extreme drift (\> 60 minutes):**

| clean_id | date | session | datetime_emo | datetime_se | drift_minutes |
|------------|------------|------------|------------|------------|------------|
| 17 | 2018-11-25 | evening | 2018-11-25 18:00:00 | 2018-11-25 21:00:00 | 180.00 |
| 27 | 2018-11-25 | evening | 2018-11-25 21:00:00 | 2018-11-25 18:00:00 | 180.00 |
| 32 | 2020-08-01 | morning | 2020-08-01 06:45:00 | 2020-08-01 08:00:10 | 75.17 |
| 173 | 2020-11-10 | afternoon | 2020-11-10 15:00:00 | 2020-11-10 16:00:01 | 60.02 |
| 173 | 2020-11-13 | afternoon | 2020-11-13 15:00:00 | 2020-11-13 16:00:01 | 60.02 |
| 173 | 2020-11-14 | afternoon | 2020-11-14 15:00:00 | 2020-11-14 16:00:01 | 60.02 |

these ids and dates may need to be flagged in downstream analyses if
session-level matching relies on tight timing

## super-users

```{r}
# check for "super users" (potential double participants or app runners)
super_users <- valid_responses %>%
  filter(total_valid_prompts > 45)

if(nrow(super_users) > 0) {
  print("warning: participants with > 45 prompts (potential duplicates/long runners):")
  print(super_users)
}

hist(
  valid_responses$total_valid_prompts,
  breaks = 20,
  col = "#3498db",
  main = "distribution of valid prompts per participant",
  xlab = "total valid surveys (max ≈ 42)"
)
abline(v = 12, col = "red", lwd = 2, lty = 2)
```

```{r}
#identify the super users
super_user_ids <- c("20", "29", "173", "178") 

print("--- gap analysis for super users ---")

for(uid in super_user_ids) {
  
  # dates for this user
  user_dates <- merged_check %>%
    filter(clean_id == uid) %>%
    arrange(date) %>%
    pull(date) %>%
    unique() %>%
    as.Date()
  
  # calc gap between each prompt
  gaps <- diff(user_dates)
  max_gap <- max(gaps)
  
  print(paste("user:", uid))
  print(paste("  date range:", min(user_dates), "to", max(user_dates)))
  print(paste("  total days active:", length(user_dates)))
  print(paste("  max gap between responses:", max_gap, "days"))
  
  if(max_gap > 7) {
    print("  -> split participation (likely t3 and t4)")
  } else {
    print("  -> consecutive (kept app running)")
  }
  print("------------------------------------------------")
}
```

## missing data & compliance

### emo & se

```{r}
# remove wave 2 data for id 178 (dates after 2020-01-01)

merged_check <- merged_check %>%
  filter(!(clean_id == "178" & date > "2020-01-01"))

print(paste(
  "fix applied. user 178 remaining rows:",
  nrow(merged_check[merged_check$clean_id == "178", ])
))
```

```{r}
# partial responses (rows where drift = NA)

partial_responses <- merged_check %>%
  filter(is.na(drift_seconds)) %>%
  mutate(
    issue_type = case_when(
      !is.na(datetime_emo) & is.na(datetime_se) ~ "missing event (se file)",
      is.na(datetime_emo) & !is.na(datetime_se) ~ "missing mood (emo file)",
      TRUE ~ "unknown"
    )
  )

print(table(partial_responses$issue_type))


# participant-level compliance

valid_responses <- merged_check %>%
  filter(!is.na(drift_seconds)) %>%
  count(clean_id, name = "total_valid_prompts") %>%
  mutate(
    compliance_rate   = round(total_valid_prompts / 42 * 100, 1),
    keep_participant  = ifelse(total_valid_prompts >= 12, "keep", "exclude (<12)")
  )

print("--- compliance summary (after fix) ---")
print(summary(valid_responses$total_valid_prompts))
print(paste(
  "participants to exclude (<12):",
  sum(valid_responses$keep_participant == "exclude (<12)")
))

super_users <- valid_responses %>%
  filter(total_valid_prompts > 45)

if (nrow(super_users) > 0) {
  print("warning: participants with > 45 prompts remaining:")
  print(super_users)
} else {
  print("success: no super users with > 45 prompts remaining")
}


# attrition across study days

attrition_df <- merged_check %>%
  filter(!is.na(drift_seconds)) %>%
  group_by(clean_id) %>%
  mutate(study_day = as.numeric(as.Date(date) - min(as.Date(date))) + 1) %>%
  ungroup() %>%
  filter(study_day >= 1)

daily_counts <- attrition_df %>%
  filter(study_day <= 30) %>%
  count(study_day)

ggplot(daily_counts, aes(study_day, n)) +
  geom_line(color = "darkred", size = 1) +
  geom_point(size = 3) +
  scale_x_continuous(breaks = seq(0, 30, 2)) +
  labs(
    title = "study attrition curve (cleaned)",
    x     = "study day",
    y     = "total responses"
  ) +
  theme_minimal()


# item-level missingness (within cleaned emo prompts)

item_check_prep <- df_emo %>%
  mutate(
    clean_id = sub("\\.2$", "", as.character(ELS_ID)),
    date     = clean_date(emo_trigger_date),
    time     = clean_time(emo_trigger_time),
    session  = get_session(time)
  )

item_check <- item_check_prep %>%
  semi_join(attrition_df, by = c("clean_id", "date", "session")) %>%
  rename(
    sad        = contains("felt.sad"),
    anxious    = contains("felt.anxious"),
    social_who = contains("who.are.you.with")
  )

missing_items <- item_check %>%
  summarize(
    total_rows      = n(),
    missing_sad     = sum(is.na(sad)),
    missing_anxious = sum(is.na(anxious)),
    missing_social  = sum(is.na(social_who))
  )

print(missing_items)
```

#### summary

review of missingness and compliance, incorporating the cleaned dataset
and removal of T4 data for participant 178, yielded the following
patterns:

**1. partial responses (n = 129)**\
unmerged emo–se pairs reflected two primary sources of missingness:

-   **missing mood (n = 84):** cases where event data was recorded but
    mood data was absent.
-   **missing event (n = 42):** cases where participants completed mood
    ratings but did not complete the event portion.\
-   **unknown (n = 3):** cases where directionality could not be clearly
    determined.

these partial responses will remain in the dataset. we will generate
flags for missing mood and missing event, but all available columns will
be retained.

**2. high-volume users (\>45 prompts)**\
after removing 2021 data for id 178, three participants still exceeded
the nominal 42-prompt window:

-   **id 120:** 62 prompts\
-   **id 129:** 47 prompts\
-   **id 173:** 47 prompts

these patterns are consistent with extended consecutive usage rather
than duplicated entries. no rows were removed; instead, study_day should
be capped at 14 during analytic processing to standardize exposure
length

**3. compliance across participants**\
valid emo–se prompt pairs ranged from 2 to 62 per participant, with:

-   **median = 28**\
-   **mean = 26.87**

**19 participants** provided fewer than 12 valid prompts

**4. item-level missingness (n = 4,152 merged prompts)**\
item completeness within finished mood surveys was very high:

-   **sad:** 8 missing\
-   **anxious:** 9 missing\
-   **social_who:** 3 missing

these counts represent \<0.2% missingness across core items. all items
will remain in the dataset, accompanied by item-level missingness flags
and counts of completed items per survey.

overall, missingness is driven primarily by skipped prompts rather than
skipped items. completed surveys show strong internal completeness, and
the cleaned dataset supports reliable downstream processing with
appropriate flags for partial responses, low-compliance participants,
high-response participants, and item-level missingness.

### sleep

```{r}
# prep sleep data (standardize id/date/deduplicate)
# we deduplicate immediately to prevent join warnings later
sleep_prep <- df_sleep %>%
  mutate(
    clean_id = sub("\\.2$", "", as.character(ELS_ID)),
    date     = clean_date(sleep_trigger_date),
    time     = clean_time(sleep_trigger_time),
    session  = get_session(time)
  ) %>%
  # sleep should only exist in "morning"
  filter(session == "morning") %>%
  # dedup: keep the last entry if duplicates exist
  group_by(clean_id, date) %>%
  slice_tail(n = 1) %>%
  ungroup() %>%
  select(clean_id, date, session, sleep_hours = contains("hours"), sleep_quality = contains("restful"))

# mismatch check: morning mood vs. sleep
# we want to know: did they do one but not the other?

# get all valid morning mood prompts (deduplicated by definition of merged_check structure)
morning_moods <- merged_check %>%
  filter(session == "morning" & !is.na(drift_seconds)) %>%
  select(clean_id, date) %>%
  distinct() %>% # Safety measure
  mutate(has_mood = 1)

# join with sleep to check mood-sleep overlap
sleep_integrity <- full_join(morning_moods, sleep_prep, by = c("clean_id", "date")) %>%
  mutate(
    type = case_when(
      !is.na(has_mood) & !is.na(sleep_quality) ~ "complete (mood + sleep)",
      !is.na(has_mood) & is.na(sleep_quality)  ~ "missing sleep (skipped?)",
      is.na(has_mood) & !is.na(sleep_quality)  ~ "sleep alone (no mood data)",
      TRUE ~ "unknown"
    )
  )

print("--- sleep vs. morning mood match rates ---")
print(table(sleep_integrity$type))

# mismatch check: morning event vs. sleep
# checking if they answered the 'significant event' question but skipped sleep
morning_events <- merged_check %>%
  filter(session == "morning" & !is.na(drift_seconds) & !is.na(datetime_se)) %>%
  select(clean_id, date) %>%
  distinct() %>%
  mutate(has_event = 1)

sleep_event_integrity <- full_join(morning_events, sleep_prep, by = c("clean_id", "date")) %>%
  mutate(
    type = case_when(
      !is.na(has_event) & !is.na(sleep_quality) ~ "complete (event + sleep)",
      !is.na(has_event) & is.na(sleep_quality)  ~ "missing sleep (skipped?)",
      is.na(has_event) & !is.na(sleep_quality)  ~ "sleep alone (no event data)",
      TRUE ~ "unknown"
    )
  )

print("--- sleep vs. morning event match rates ---")
print(table(sleep_event_integrity$type))

# identify "sleep skippers"
# participants who do the morning survey but frequently skip the sleep questions
sleep_skippers <- sleep_integrity %>%
  filter(type == "missing sleep (skipped?)") %>%
  count(clean_id) %>%
  arrange(desc(n))

if(nrow(sleep_skippers) > 0) {
  print("--- top participants missing sleep data ---")
  print(head(sleep_skippers))
}
```

#### summary

morning sleep entries were evaluated against morning mood and morning
event data to determine whether participants were consistently
completing all components of the daily protocol. after deduplication of
the sleep file and removal of extended-day cases, three patterns
emerged.

**1. sleep vs. morning mood overlap**\
the majority of prompts contained both a valid morning mood rating and a
corresponding sleep entry:

-   **complete (mood + sleep): 1,262 cases**\
-   **missing sleep (skipped?): 15 cases**\
-   **sleep alone (no mood data): 58 cases**

the “missing sleep” category indicates moments where participants
completed the mood portion but did not answer the sleep items. the
“sleep alone” category likely reflects participants entering sleep data
without responding to the morning prompt, or cases where mood entries
were lost before merging.

**2. sleep vs. morning event overlap**\
results were nearly identical when using event reports rather than mood
reports as the anchor:

-   **complete (event + sleep): 1,262 cases**\
-   **missing sleep (skipped?): 15 cases**\
-   **sleep alone (no event data): 58 cases**

this confirms that sleep reporting behaves consistently relative to both
morning mood and event data, with the same small subset of skipped or
orphaned entries.

**3. identification of sleep skippers**\
a small number of participants repeatedly completed the morning mood
survey but skipped the sleep section:

| clean_id | missing_sleep_count |
|----------|---------------------|
| 214      | 3                   |
| 176      | 2                   |
| 113      | 1                   |
| 114      | 1                   |
| 120      | 1                   |
| 121      | 1                   |

although the counts are low, these ids can be flagged for optional
sensitivity checks.

**summary**\
morning sleep reporting is highly consistent with morning mood and event
data, with only \~1% of entries missing sleep and \~4% containing sleep
only responses.

### counts

```{r}
# temp fix: user 178 t4 removal
clean_merged_temp <- merged_check %>%
  filter(!(clean_id == "178" & date > "2020-01-01"))

# temp fix: sleep file (clean ids, rm user 178 wave 2, dedup)
clean_sleep_temp <- df_sleep %>%
  mutate(clean_id = sub("\\.2$", "", as.character(ELS_ID))) %>%
  filter(!(clean_id == "178" & sleep_trigger_date > "2020-01-01")) %>%
  mutate(date = clean_date(sleep_trigger_date)) %>% 
  group_by(clean_id, date) %>%
  slice_tail(n = 1) %>% 
  ungroup()

# count mood/stress/paired
main_counts <- clean_merged_temp %>%
  group_by(clean_id) %>%
  summarize(
    n_mood   = sum(!is.na(datetime_emo)),
    n_stress = sum(!is.na(datetime_se)),
    n_paired = sum(!is.na(datetime_emo) & !is.na(datetime_se))
  )

# count sleep
sleep_counts <- clean_sleep_temp %>%
  count(clean_id, name = "n_sleep")

# combine 
data_report <- full_join(main_counts, sleep_counts, by = "clean_id") %>%
  replace_na(list(n_mood = 0, n_stress = 0, n_paired = 0, n_sleep = 0)) %>%
  arrange(as.numeric(clean_id))

# output
print("--- top 10 ---")
print(head(data_report, 10))

print("--- summary statistics ---")
print(summary(data_report[, -1]))

# check: mood present but sleep absent?
sleep_missing <- data_report %>% filter(n_mood > 10 & n_sleep == 0)

if(nrow(sleep_missing) > 0) {
  print("warning: participants with mood data but zero sleep data:")
  print(sleep_missing)
} else {
  print("check passed: all mood participants have sleep data")
}
```

####  summary

counts were computed for each participant across four domains: mood
entries (`n_mood`), stress/event entries (`n_stress`), fully paired
mood–event prompts (`n_paired`), and morning sleep reports (`n_sleep`).
these metrics reflect overall adherence and the relative completeness of
each module.

**summary statistics across participants**

| metric   | min | 1st quartile | median | mean  | 3rd quartile | max |
|----------|-----|--------------|--------|-------|--------------|-----|
| n_mood   | 0   | 17           | 28     | 26.47 | 37           | 63  |
| n_stress | 0   | 17           | 29     | 26.73 | 37           | 62  |
| n_paired | 0   | 16           | 28     | 26.20 | 37           | 62  |
| n_sleep  | 0   | 4            | 9      | 8.10  | 12           | 19  |

overall, mood and stress completion rates were highly similar, with most
participants completing between 17 and 37 prompts across the 14-day
window. paired responses (where both mood and event were recorded)
aligned closely with these values. morning sleep entries were less
frequent by design (1 per day), with typical participants providing
between 4 and 12 sleep reports.

**participants with mood data but zero sleep entries**

a set of participants completed mood surveys but never submitted morning
sleep data. these cases are noteworthy because sleep entries are
expected at least once per day.

| clean_id | n_mood | n_stress | n_paired | n_sleep |
|----------|--------|----------|----------|---------|
| 216      | 16     | 16       | 16       | 0       |
| 163      | 35     | 28       | 28       | 0       |
| 282      | 29     | 28       | 28       | 0       |
| 289      | 28     | 28       | 28       | 0       |
| 63       | 9      | 4        | 4        | 0       |
| 94       | 13     | 13       | 12       | 0       |
| 139      | 31     | 31       | 31       | 0       |
| 131      | 21     | 21       | 21       | 0       |
| 217      | 17     | 17       | 17       | 0       |
| 171      | 17     | 17       | 17       | 0       |
| 113      | 12     | 12       | 12       | 0       |
| 121      | 12     | 12       | 12       | 0       |
| 241      | 14     | 14       | 14       | 0       |
| 252      | 5      | 5        | 5        | 0       |
| 78       | 15     | 15       | 15       | 0       |
| 153      | 33     | 33       | 33       | 0       |
| 38       | 8      | 8        | 8        | 0       |
| 42       | 13     | 13       | 13       | 0       |
| 214      | 20     | 20       | 20       | 0       |

additionally, one participant, **id 114**, submitted stress data
(n_stress = 30) but recorded no sleep entries (`n_sleep = 0`). sleep
absence for these cases will be flagged but not removed.

## effort

```{r}
# 1. process emo (mood) - fixing column mapping for raw data
emo_diag <- df_emo %>%
  mutate(
    clean_id = sub("\\.2$", "", as.character(ELS_ID)),
    date     = clean_date(emo_trigger_date),
    time     = clean_time(emo_trigger_time),
    session  = get_session(time),
    
    # extract numeric items from raw long names
    sad       = as.numeric(gsub("[^0-9]", "", `Since.the.last.prompt..please.indicate.the.extent.to.which.you.felt.sad`)),
    angry     = as.numeric(gsub("[^0-9]", "", `Since.the.last.prompt..please.indicate.the.extent.to.which.you.felt.annoyed.angry`)),
    grouchy   = as.numeric(gsub("[^0-9]", "", `Since.the.last.prompt..please.indicate.the.extent.to.which.you.felt.grouchy.cranky`)),
    worried   = as.numeric(gsub("[^0-9]", "", `Since.the.last.prompt..please.indicate.the.extent.to.which.you.felt.worried`)),
    anxious   = as.numeric(gsub("[^0-9]", "", `Since.the.last.prompt..please.indicate.the.extent.to.which.you.felt.anxious`)),
    happy     = as.numeric(gsub("[^0-9]", "", `Since.the.last.prompt..please.indicate.the.extent.to.which.you.felt.happy`)),
    cheerful  = as.numeric(gsub("[^0-9]", "", `Since.the.last.prompt..please.indicate.the.extent.to.which.you.felt.cheerful`)),
    excited   = as.numeric(gsub("[^0-9]", "", `Since.the.last.prompt..please.indicate.the.extent.to.which.you.felt.excited`)),
    energetic = as.numeric(gsub("[^0-9]", "", `Since.the.last.prompt..please.indicate.the.extent.to.which.you.felt.energetic`)),
    
    # timing
    t_start = clean_time(emo_survey_start_time),
    t_end   = clean_time(emo_survey_submit_time)
  ) %>%
  filter(!(clean_id == "178" & date > "2020-01-01")) %>%
  group_by(clean_id, date, session) %>% 
  slice_tail(n = 1) %>% 
  ungroup()

# 2. process se (event)
se_diag <- df_se %>%
  mutate(
    clean_id = sub("\\.2$", "", as.character(ELS_ID)),
    date     = clean_date(se_trigger_date),
    time     = clean_time(se_trigger_time),
    session  = get_session(time),
    
    # map raw columns
    event_occurred = `Since.the.last.prompt..have.you.experienced.a.significant.event.`,
    stress_raw     = `How.stressful.was.the.event.`
  ) %>%
  filter(!(clean_id == "178" & date > "2020-01-01")) %>%
  group_by(clean_id, date, session) %>% 
  slice_tail(n = 1) %>% 
  ungroup()

# 3. process sleep
sleep_diag <- df_sleep %>%
  mutate(
    clean_id = sub("\\.2$", "", as.character(ELS_ID)),
    date     = clean_date(sleep_trigger_date),
    time     = clean_time(sleep_trigger_time),
    session  = get_session(time),
    
    t_start_sleep = clean_time(sleep_survey_start_time),
    t_end_sleep   = clean_time(sleep_survey_submit_time)
  ) %>%
  filter(session == "morning" & !(clean_id == "178" & date > "2020-01-01")) %>%
  group_by(clean_id, date) %>% 
  slice_tail(n = 1) %>% 
  ungroup()

# 4. merge and calculate effort flags
diag_master <- full_join(emo_diag, se_diag, by = c("clean_id", "date", "session")) %>%
  left_join(sleep_diag, by = c("clean_id", "date"))

diag_flags <- diag_master %>%
  rowwise() %>%
  mutate(
    # duration effort (mood)
    dur_main = as.numeric(difftime(ymd_hms(paste(date, t_end)), 
                                   ymd_hms(paste(date, t_start)), units = "secs")),
    flag_duration = ifelse(!is.na(dur_main) & dur_main < 20, 1, 0),
    
    # na effort (straightlining negative affect)
    sd_na = sd(c(sad, angry, grouchy, worried, anxious), na.rm = TRUE),
    mean_na = mean(c(sad, angry, grouchy, worried, anxious), na.rm = TRUE),
    flag_na_effort = ifelse(!is.na(sd_na) & sd_na == 0 & mean_na > 0, 1, 0),
    
    # pa effort (straightlining positive affect)
    sd_pa = sd(c(happy, cheerful, excited, energetic), na.rm = TRUE),
    mean_pa = mean(c(happy, cheerful, excited, energetic), na.rm = TRUE),
    flag_pa_effort = ifelse(!is.na(sd_pa) & sd_pa == 0 & mean_pa > 0, 1, 0),
    
    # sleep effort (duration)
    dur_sleep = as.numeric(difftime(ymd_hms(paste(date, t_end_sleep)), 
                                    ymd_hms(paste(date, t_start_sleep)), units = "secs")),
    flag_sleep_effort = ifelse(!is.na(dur_sleep) & dur_sleep < 4, 1, 0),
    
    # event effort (consistency)
    flag_event_effort = ifelse(event_occurred == "Yes" & is.na(stress_raw), 1, 0)
  ) %>%
  ungroup()

# 5. summarize by participant
systematic_check <- diag_flags %>%
  group_by(clean_id) %>%
  summarize(
    total_prompts = n(),
    bad_speed = sum(flag_duration, na.rm = TRUE),
    bad_na = sum(flag_na_effort, na.rm = TRUE),
    bad_pa = sum(flag_pa_effort, na.rm = TRUE),
    bad_sleep = sum(flag_sleep_effort, na.rm = TRUE),
    bad_event = sum(flag_event_effort, na.rm = TRUE),
    pct_low_effort = round((bad_speed + bad_na) / total_prompts * 100, 1)
  ) %>%
  arrange(desc(pct_low_effort))

# print diagnostics
print("--- effort: top participants ---")
print(head(systematic_check, 10))

print("--- effort summary flags ---")
print(paste("fast (<20s) mood entries:", sum(diag_flags$flag_duration, na.rm = TRUE)))
print(paste("straightlined negative affect:", sum(diag_flags$flag_na_effort, na.rm = TRUE)))
print(paste("straightlined positive affect:", sum(diag_flags$flag_pa_effort, na.rm = TRUE)))
print(paste("very fast sleep entries (<4s):", sum(diag_flags$flag_sleep_effort, na.rm = TRUE)))
print(paste("inconsistent event reports:", sum(diag_flags$flag_event_effort, na.rm = TRUE)))
```

```{r}
# 1. isolate straight-liners (sd=0 but mean > 0)
na_straight_liners <- diag_flags %>%
  filter(flag_na_effort == 1) %>%
  # removed 'session' to avoid .x/.y errors
  select(clean_id, date, sad) %>% 
  mutate(chosen_value = sad)

pa_straight_liners <- diag_flags %>%
  filter(flag_pa_effort == 1) %>%
  select(clean_id, date, happy) %>%
  mutate(chosen_value = happy)

# 2. frequency table: what numbers are they picking?
print("--- negative affect: values chosen ---")
print(table(na_straight_liners$chosen_value))

print("--- positive affect: values chosen ---")
print(table(pa_straight_liners$chosen_value))

# 3. check "lazy" midpoint (50) vs "crisis" (80+)
lazy_na <- na_straight_liners %>% filter(chosen_value == 50)
high_na <- na_straight_liners %>% filter(chosen_value > 80)

print(paste("midpoint (50) entries:", nrow(lazy_na)))
print(paste("high intensity (>80) entries:", nrow(high_na)))

if(nrow(high_na) > 0) {
  print("--- potential valid distress (high intensity) ---")
  print(head(high_na))
}
```

### summary

effort checks were applied to identify prompts completed unusually
quickly or with no variability across affect items. these checks
explicitly allow for the possibility that participants may legitimately
report very low or zero affect. flags were only applied when all items
in a domain were identical **and greater than zero**, ensuring that
valid low-arousal states (e.g., all zeros or all ones) were not mistaken
for low-effort responding.

**distribution of straight-lined values**

an additional inspection of straight-lined prompts showed that the vast
majority reflected consistent emotional states rather than default or
midpoint responding:

-   for negative affect, **446 of 451** straight-lined prompts used the
    value **1**, and **2 prompts** used **100**\
-   no straight-lined prompts used midpoints such as 50\
-   for positive affect, straight-lined values clustered at **1** and
    **100**, with few intermediate values

the absence of midpoint straight-lining suggests participants were
actively selecting values rather than clicking through defaults. entries
at the high end (100) may reflect acute emotional states, and entries at
the low end (1) may reflect calm or neutral states. all straight-lined
entries are therefore retained.

**summary of effort indicators**

-   fast mood entries (\<20 seconds): **11**\
-   straightlined negative affect (\>0 and identical): **451**\
-   straightlined positive affect (\>0 and identical): **136**\
-   very fast sleep entries (\<4 seconds): **7**\
-   inconsistent event reports: **0**

non-zero straightlining was the most common pattern, particularly for
negative affect. rapid completions were rare, and no cases were observed
where participants endorsed an event but skipped the required stress
rating.

**participants with the highest proportion of flagged responses**

| clean_id | total_prompts | bad_na | bad_pa | bad_sleep | pct_low_effort |
|----------|---------------|--------|--------|-----------|----------------|
| 184      | 29            | 28     | 26     | 2         | 96.6%          |
| 96       | 34            | 31     | 1      | 0         | 91.2%          |
| 69       | 15            | 13     | 2      | 0         | 86.7%          |
| 123      | 19            | 16     | 0      | 0         | 84.2%          |
| 189      | 36            | 30     | 11     | 0         | 83.3%          |
| 97       | 39            | 29     | 0      | 0         | 74.4%          |
| 55       | 32            | 19     | 0      | 0         | 59.4%          |
| 152      | 24            | 13     | 0      | 0         | 54.2%          |
| 167      | 42            | 22     | 5      | 0         | 52.4%          |
| 186      | 32            | 16     | 0      | 0         | 50.0%          |

**overall interpretation**

these checks show that most flagged patterns reflect consistent
affective states rather than mechanical clicking or invalid data.
straight-lined entries overwhelmingly used meaningful values (e.g., 1 or
100), not default midpoints. all entries and all effort flags are
retained for potential sensitivity analyses, but no prompts or
participants are removed at this stage.

# file export with flags

given this raw data review, the following code addresses technical
inconsistencies and identifies potentially careless responding using
empirically informed thresholds from Jaso et al., 2022 and Portillo-Van
Diest et al., 2024.

Diest, A. P.-V., Mortier, P., Ballester, L., Amigo, F., Carrasco, P.,
Falcó, R., Gili, M., Kiekens, G., Machancoses, F. H., Piqueras, J. A.,
Rebagliato, M., Roca, M., Rodríguez-Jiménez, T., Alonso, J., & Vilagut,
G. (2024). Ecological Momentary Assessment of Mental Health Problems
Among University Students: Data Quality Evaluation Study. *Journal of
Medical Internet Research*, *26*(1), e55712.
<https://doi.org/10.2196/55712>

Jaso, B. A., Kraus, N. I., & Heller, A. S. (2022). Identification of
careless responding in ecological momentary assessment research: From
posthoc analyses to real-time data monitoring.*Psychological Methods,
27*(6), 958–981.
<https://doi-org.stanford.idm.oclc.org/10.1037/met0000312>

```{r}
print("--- df_emo (Daily Emotion Data Columns) ---")
print(colnames(df_emo))

print("--- df_se (Daily Significant Event Data Columns) ---")
print(colnames(df_se))

print("--- df_sleep (Daily Sleep Data Columns) ---")
print(colnames(df_sleep))
```

```{r}
library(tidyverse)
library(lubridate)

base_path  <- "/Users/eu/Documents/GitHub/els_ema_data/data/raw/" 
df_emo     <- read.csv(paste0(base_path, "2_daily_emo_total_data.csv"), stringsAsFactors = FALSE) 
df_se      <- read.csv(paste0(base_path, "2_daily_se_total_data.csv"), stringsAsFactors = FALSE)
df_sleep   <- read.csv(paste0(base_path, "2_daily_sleep_total_data.csv"), stringsAsFactors = FALSE)

#clean functions
clean_time <- function(x) {
  sapply(x, function(t) {
    if (is.na(t) || t == "") return(NA_character_)
    if (grepl("^[0-9]+$", t)) { 
      sec <- as.numeric(t)
      sprintf("%02d:%02d:%02d", sec %/% 3600, (sec %% 3600) %/% 60, sec %% 60)
    } else {
      out <- tryCatch(
        format(parse_date_time(t, c("HMS", "HM")), "%H:%M:%S"),
        error = function(e) NA_character_
      )
      out
    }
  })
}

clean_date <- function(x) {
  format(parse_date_time(x, orders = c("dmy", "mdy", "ymd", "Ymd", "dmY")), "%Y-%m-%d")
}

get_session <- function(time_str) {
  h <- hour(hms(time_str))
  case_when(h >= 4 & h < 14 ~ "morning", h >= 14 & h < 18 ~ "afternoon", h >= 18 ~ "evening", TRUE ~ NA_character_)
}

calc_modal_pct <- function(x) {
  vals <- x[!is.na(x)]
  if(length(vals) == 0) return(NA_real_)
  max(table(vals)) / length(vals)
}

# item lists
num_mood_items <- 9
num_event_items <- 3

# 2. clean and prepare individual files

# process emo (mood)
emo_clean <- df_emo %>%
  mutate(
    clean_id = sub("\\.2$", "", as.character(ELS_ID)),
    date     = clean_date(emo_trigger_date),
    session  = get_session(clean_time(emo_trigger_time)),
    
    # extracted numeric response items 
    emo_sad       = as.numeric(gsub("[^0-9]", "", `Since.the.last.prompt..please.indicate.the.extent.to.which.you.felt.sad`)),
    emo_angry     = as.numeric(gsub("[^0-9]", "", `Since.the.last.prompt..please.indicate.the.extent.to.which.you.felt.annoyed.angry`)),
    emo_grouchy   = as.numeric(gsub("[^0-9]", "", `Since.the.last.prompt..please.indicate.the.extent.to.which.you.felt.grouchy.cranky`)),
    emo_worried   = as.numeric(gsub("[^0-9]", "", `Since.the.last.prompt..please.indicate.the.extent.to.which.you.felt.worried`)),
    emo_anxious   = as.numeric(gsub("[^0-9]", "", `Since.the.last.prompt..please.indicate.the.extent.to.which.you.felt.anxious`)),
    emo_happy     = as.numeric(gsub("[^0-9]", "", `Since.the.last.prompt..please.indicate.the.extent.to.which.you.felt.happy`)),
    emo_cheerful  = as.numeric(gsub("[^0-9]", "", `Since.the.last.prompt..please.indicate.the.extent.to.which.you.felt.cheerful`)),
    emo_excited   = as.numeric(gsub("[^0-9]", "", `Since.the.last.prompt..please.indicate.the.extent.to.which.you.felt.excited`)),
    emo_energetic = as.numeric(gsub("[^0-9]", "", `Since.the.last.prompt..please.indicate.the.extent.to.which.you.felt.energetic`)),
    
    # timing (for duration calc)
    t_start_emo  = clean_time(emo_survey_start_time),
    t_end_emo    = clean_time(emo_survey_submit_time)
  ) %>%
  filter(!(clean_id == "178" & date > "2020-01-01")) %>% 
  group_by(clean_id, date, session) %>% slice_tail(n = 1) %>% ungroup() %>%
  select(
    clean_id, date, session, 
    # timing
    t_start_emo, t_end_emo, emo_trigger_time, emo_trigger_date,
    # renamed items
    emo_sad, emo_angry, emo_grouchy, emo_worried, emo_anxious, 
    emo_happy, emo_cheerful, emo_excited, emo_energetic,
    emo_bored = `Since.the.last.prompt..please.indicate.the.extent.to.which.you.felt.bored`,
    emo_want_people = `Since.the.last.prompt..please.indicate.the.extent.to.which.you.want.to.be.with.other.people`,
    emo_want_alone = `Since.the.last.prompt..please.indicate.the.extent.to.which.you.want.to.be.alone`,
    emo_who_with = `Right.now..who.are.you.with.`,
    emo_face_to_face = `Right.now..are.you.having.a.face.to.face.conversation.`,
    emo_digital_conv = `Right.now..are.you.having.a.real.time.digital..phone..text..Facebook..video..conversation.`
  )

# process se (event)
se_clean <- df_se %>%
  mutate(
    clean_id = sub("\\.2$", "", as.character(ELS_ID)),
    date     = clean_date(se_trigger_date),
    session  = get_session(clean_time(se_trigger_time)),
    t_start_event = clean_time(se_survey_start_time),
    t_end_event   = clean_time(se_survey_submit_time),
    
    event_occurred_clean = tolower(trimws(`Since.the.last.prompt..have.you.experienced.a.significant.event.`)), 
    
    # extract event sliders and convert to numeric
    event_stress_num   = suppressWarnings(as.numeric(`How.stressful.was.the.event.`)),
    event_control_num  = suppressWarnings(as.numeric(`How.able.were.you.to.control.the.event.`)),
    event_importance_num = suppressWarnings(as.numeric(`How.important.was.the.event.`))
  ) %>%
  filter(!(clean_id == "178" & date > "2020-01-01")) %>%
  group_by(clean_id, date, session) %>% slice_tail(n = 1) %>% ungroup() %>%
  select(
    clean_id, date, session, 
    # timing
    t_start_event, t_end_event, se_trigger_time, se_trigger_date,
    # renamed items and original raw response strings
    event_significant = `Since.the.last.prompt..have.you.experienced.a.significant.event.`,
    event_stress_raw = `How.stressful.was.the.event.`,
    event_importance_raw = `How.important.was.the.event.`,
    event_control_raw = `How.able.were.you.to.control.the.event.`,
    # clean items 
    event_occurred_clean, event_stress_num, event_control_num, event_importance_num
  )

# process sleep
sleep_clean <- df_sleep %>%
  mutate(
    clean_id = sub("\\.2$", "", as.character(ELS_ID)),
    date     = clean_date(sleep_trigger_date),
    session  = get_session(clean_time(sleep_trigger_time)),
    t_start_sleep = clean_time(sleep_survey_start_time),
    t_end_sleep   = clean_time(sleep_survey_submit_time),
    
    # extracted numeric response items
    sleep_quality = as.numeric(gsub("[^0-9]", "", `How.restful.or.satisfying.was.your.sleep.last.night.`)),
    sleep_hours   = as.numeric(gsub("[^0-9]", "", `How.many.hours.did.you.sleep.last.night.`))
  ) %>%
  filter(session == "morning" & !(clean_id == "178" & date > "2020-01-01")) %>%
  group_by(clean_id, date) %>% slice_tail(n = 1) %>% ungroup() %>%
  select(
    clean_id, date, session, 
    # timing
    t_start_sleep, t_end_sleep, sleep_trigger_time, sleep_trigger_date,
    # renamed items
    sleep_quality, sleep_hours
  )


# 3. merge all files and calculate flags
master_merged <- full_join(emo_clean, se_clean, by = c("clean_id", "date", "session")) %>%
  left_join(sleep_clean, by = c("clean_id", "date", "session"))

# calculate study day and apply max filter
master_merged <- master_merged %>%
  filter(!is.na(date)) %>% 
  group_by(clean_id) %>%
  mutate(study_day = as.numeric(as.Date(date) - min(as.Date(date), na.rm = TRUE)) + 1) %>%
  ungroup() %>%
  filter(study_day <= 16)

ema_flags_master <- master_merged %>%
  rowwise() %>%
  mutate(
    # duration calculations 
    dt_end_emo    = suppressWarnings(ymd_hms(paste(date, t_end_emo))),
    dt_start_emo  = suppressWarnings(ymd_hms(paste(date, t_start_emo))),
    dt_end_event   = suppressWarnings(ymd_hms(paste(date, t_end_event))),
    dt_start_event = suppressWarnings(ymd_hms(paste(date, t_start_event))),
    dt_end_sleep   = suppressWarnings(ymd_hms(paste(date, t_end_sleep))),
    dt_start_sleep = suppressWarnings(ymd_hms(paste(date, t_start_sleep))),

    dur_mood     = as.numeric(difftime(dt_end_emo, dt_start_emo, units="secs")),
    dur_event    = as.numeric(difftime(dt_end_event, dt_start_event, units="secs")),
    dur_sleep    = as.numeric(difftime(dt_end_sleep, dt_start_sleep, units="secs")),
    
    # prompt-level metrics
    mood_vector  = list(c_across(c(emo_sad, emo_angry, emo_grouchy, emo_worried, emo_anxious, emo_happy, emo_cheerful, emo_excited, emo_energetic))),
    mood_mean    = mean(unlist(mood_vector), na.rm=TRUE),
    mood_sd      = sd(unlist(mood_vector), na.rm=TRUE),
    mood_mode_prop= calc_modal_pct(unlist(mood_vector)),
    
    event_vector = list(c(event_stress_num, event_control_num, event_importance_num)),
    event_mean   = mean(unlist(event_vector), na.rm=TRUE),
    event_sd     = sd(unlist(event_vector), na.rm=TRUE),
    event_mode_prop= calc_modal_pct(unlist(event_vector)),

    # 1. mood flags
    flag_speed_mood  = ifelse(!is.na(dur_mood) & dur_mood <= num_mood_items, 1L, 0L),
    flag_var_mood    = ifelse(!is.na(mood_sd) & mood_sd <= 5 & mood_mean > 10, 1L, 0L),
    flag_modal_mood  = ifelse(!is.na(mood_mode_prop) & mood_mode_prop >= 0.60 & mood_mean > 10, 1L, 0L),
    any_flag_mood    = ifelse(flag_speed_mood==1 | flag_var_mood==1 | flag_modal_mood==1, 1L, 0L),
    
    # 2. event flags
    flag_event_speed = ifelse(!is.na(dur_event) & dur_event <= num_event_items & event_occurred_clean == "yes", 1L, 0L),
    flag_event_var   = ifelse(!is.na(event_sd) & event_sd <= 5 & event_mean > 10, 1L, 0L),
    flag_event_modal = ifelse(!is.na(event_mode_prop) & event_mode_prop >= 0.60 & event_mean > 10, 1L, 0L),
    any_flag_event   = ifelse(flag_event_speed==1 | flag_event_var==1 | flag_event_modal==1, 1L, 0L),
    
    # 3. sleep flags
    flag_sleep_effort  = ifelse(!is.na(dur_sleep) & dur_sleep < 4, 1L, 0L),
    flag_sleep_bounds  = ifelse(!is.na(sleep_hours) & (sleep_hours < 0 | sleep_hours > 18), 1L, 0L),
    any_flag_sleep     = ifelse(flag_sleep_effort==1 | flag_sleep_bounds==1, 1L, 0L),
    
    # 4. missingness and drift 
  flag_missing_mood  = ifelse(!is.na(event_occurred_clean) & is.na(mood_mean), 1L, 0L),
    flag_missing_event = ifelse(!is.na(mood_mean) & is.na(event_occurred_clean), 1L, 0L),
    flag_event_inconsistent = ifelse(event_occurred_clean == "yes" & is.na(event_stress_num), 1L, 0L), 

    drift_seconds      = abs(as.numeric(difftime(dt_start_emo, dt_start_event, units="secs"))),
    flag_large_drift   = ifelse(!is.na(drift_seconds) & drift_seconds >= 3600, 1L, 0L)
  ) %>%
  ungroup() %>%
  
  # 5. participant-level counts
  group_by(clean_id) %>%
  mutate(
    n_valid_mood_raw  = sum(any_flag_mood == 0 & flag_missing_mood == 0, na.rm=TRUE),
    n_valid_event_raw = sum(any_flag_event == 0 & flag_missing_event == 0 & flag_event_inconsistent == 0, na.rm=TRUE),
    n_valid_sleep_raw = sum(any_flag_sleep == 0 & !is.na(sleep_hours), na.rm=TRUE)
  ) %>%
  ungroup() %>%
  
  # 6. final output selection
  select(
    clean_id, date, session, study_day, 
    # drift and timing
    drift_seconds, flag_large_drift,
    t_start_emo, t_end_emo, t_start_event, t_end_event, t_start_sleep, t_end_sleep,
    # missingness flags
    flag_missing_mood, flag_missing_event, flag_event_inconsistent,
    # jaso and effort flags
    flag_speed_mood, flag_var_mood, flag_modal_mood, any_flag_mood,
    flag_event_speed, flag_event_var, flag_event_modal, any_flag_event,
    flag_sleep_effort, flag_sleep_bounds, any_flag_sleep,
    # n valid counts
    n_valid_mood_raw, n_valid_event_raw, n_valid_sleep_raw,
    # calculated means 
    mood_mean, mood_sd, event_mean, event_sd, 
    # emo items
    emo_sad, emo_angry, emo_grouchy, emo_worried, emo_anxious, emo_happy, emo_cheerful, emo_excited, emo_energetic, 
    emo_bored, emo_want_people, emo_want_alone, emo_who_with, emo_face_to_face, emo_digital_conv,
    # event items 
    event_occurred_clean, event_significant, event_stress_raw, event_importance_raw, event_control_raw,
    # sleep items
    sleep_quality, sleep_hours
  )

print("--- counts---")
print(summary(ema_flags_master %>% select(n_valid_event_raw, n_valid_mood_raw)))
```

```{r}
total_participants <- n_distinct(ema_flags_master$clean_id)
total_rows <- nrow(ema_flags_master)

print(paste(total_participants))
print(paste(total_rows))
```



